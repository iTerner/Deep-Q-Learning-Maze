{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import stat\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.special as sp\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"working on {device}\")\n",
    "\n",
    "Transition = collections.namedtuple(\"Experience\", field_names=[\n",
    "                                    \"state\", \"action\", \"next_state\", \"reward\", \"is_game_on\"])\n",
    "\n",
    "SAVE_PATH = \"res\"\n",
    "MODEL_PATH = \"RL_models\"\n",
    "SOL_PATH = \"sol\"\n",
    "p = [SAVE_PATH, MODEL_PATH, SOL_PATH]\n",
    "for path in p:\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    Transition = collections.namedtuple(\"Experience\", field_names=[\n",
    "                                        \"state\", \"action\", \"next_state\", \"reward\", \"is_game_on\"])\n",
    "    def __init__(self, maze, memory_buffer, device, use_softmax=True):\n",
    "        self.env = maze  # the environment (in our case this is a maze)\n",
    "        self.buffer = memory_buffer  # this is actually a reference\n",
    "        self.num_act = 4  # number of actions possible\n",
    "        self.use_softmax = use_softmax  # whether to use softmax or not\n",
    "        self.total_reward = 0  # total reward\n",
    "        self.min_reward = -self.env.maze.size  # minimum reward\n",
    "        self.is_game_on = True  # whether the game is still running or not\n",
    "        self.device = device  # the device we are using (cpu or cuda)\n",
    "\n",
    "    def make_move(self, net, epsilon, device):\n",
    "        # select an action\n",
    "        action = self.select_action(net, epsilon, device)\n",
    "        current_state = self.env.state()\n",
    "        next_state, reward, self.is_game_on = self.env.update_state(action)\n",
    "        # add to the total reward\n",
    "        self.total_reward += reward\n",
    "\n",
    "        if self.total_reward < self.min_reward:  # stop the game in this case\n",
    "            self.is_game_on = False\n",
    "        if not self.is_game_on:\n",
    "            self.total_reward = 0  # reset the total reward\n",
    "\n",
    "        transition = self.Transition(current_state, action,\n",
    "                                next_state, reward, self.is_game_on)\n",
    "\n",
    "        self.buffer.push(transition)\n",
    "\n",
    "    def select_action(self, net, epsilon, device):\n",
    "        state = torch.Tensor(self.env.state()).to(device).view(1, -1)\n",
    "        qvalues = net(state).cpu().detach().numpy().squeeze()\n",
    "\n",
    "        # softmax sampling of the qvalues\n",
    "        if self.use_softmax:\n",
    "            p = sp.softmax(qvalues / epsilon).squeeze()\n",
    "            p /= np.sum(p)\n",
    "            action = np.random.choice(self.num_act, p=p)\n",
    "        else:\n",
    "            # else choose the best action with probability 1-epsilon\n",
    "            # and with probability epsilon choose at random\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(self.num_act, size=1)[0]\n",
    "            else:\n",
    "                action = np.argmax(qvalues, axis=0)\n",
    "                action = int(action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def plot_policy_map(self, net, filename, offset):\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.imshow(self.env.maze, \"Greys\")\n",
    "\n",
    "            # loop on all the allowed cells\n",
    "            for free_cell in self.env.allowed_states:\n",
    "                # get the action\n",
    "                self.env.current_pos = np.asarray(free_cell)\n",
    "                qvalues = net(torch.Tensor(self.env.state()\n",
    "                                           ).view(1, -1).to(self.device))\n",
    "                action = int(torch.argmax(qvalues).detach().cpu().numpy())\n",
    "                # get the policy\n",
    "                policy = self.env.directions[action]\n",
    "\n",
    "                ax.text(free_cell[1] - offset[0],\n",
    "                        free_cell[0]-offset[1], policy)\n",
    "\n",
    "            ax = plt.gca()\n",
    "            plt.xticks([], [])\n",
    "            plt.yticks([], [])\n",
    "            ax.plot(self.env.goal[1], self.env.goal[0], \"bs\", markersize=4)\n",
    "            plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnvironment:\n",
    "    def __init__(self, maze, init_pos, goal):\n",
    "        x, y = len(maze), len(maze)\n",
    "\n",
    "        self.bound = np.asarray([x, y])\n",
    "        self.maze = maze  # the maze\n",
    "        self.init_pos = init_pos  # the initial position in the maze\n",
    "        self.goal = goal  # the goal position in the maze\n",
    "        # the current position, initalize to the start position\n",
    "        self.current_pos = np.asarray(init_pos)\n",
    "\n",
    "        self.visited = set()  # a set of all the visited positions in the maze\n",
    "        self.visited.add(tuple(self.current_pos))\n",
    "\n",
    "        # initialize the empty cells and the distance (euclidean) from the goal\n",
    "        # and removing the goal cell\n",
    "        self.allowed_states = np.asarray(np.where(self.maze == 0)).T.tolist()\n",
    "        self.distances = np.sqrt(\n",
    "            np.sum((np.array(self.allowed_states) - np.asarray(self.goal)) ** 2, axis=1))\n",
    "\n",
    "        del(self.allowed_states[np.where(self.distances == 0)[0][0]])\n",
    "        self.distances = np.delete(self.distances, np.where(self.distances == 0)[0][0])\n",
    "\n",
    "\n",
    "        # define action map\n",
    "        # the agent has 4 possible actions: go right/left/down/up\n",
    "        self.action_map = {\n",
    "            0: [0, 1],  # right\n",
    "            1: [0, -1],  # left\n",
    "            2: [1, 0],  # down\n",
    "            3: [-1, 0]  # up\n",
    "        }\n",
    "\n",
    "        self.directions = {\n",
    "            0: \"→\",\n",
    "            1: \"←\",\n",
    "            2: \"↓ \",\n",
    "            3: \"↑\"\n",
    "        }\n",
    "\n",
    "    def reset_policy(self, eps, reg=7):\n",
    "        \"\"\"\n",
    "        The function reset the policy, so that for high epsilon the inital position is \n",
    "        nearer to the goal (very useful for large mazes)\n",
    "        Args:\n",
    "            eps - the epsilon value\n",
    "            reg = regularization value (default 7)\n",
    "        Return:\n",
    "            reset policy\n",
    "        \"\"\"\n",
    "        return sp.softmax(-self.distances / (reg * (1 - eps ** (2 / reg))) ** (reg / 2)).squeeze()\n",
    "\n",
    "    def reset(self, epsilon, prand=0):\n",
    "        \"\"\"\n",
    "        The function reset the environment when the game is completed with a given probability.\n",
    "        Args:\n",
    "            epsilon - the epsilon value\n",
    "            prnad - the probability value for the reset to be random, otherwise, the reset policy\n",
    "            at the given epsilon is used\n",
    "        Return:\n",
    "            reset environment \n",
    "        \"\"\"\n",
    "        # random reset\n",
    "        if np.random.rand() < prand:\n",
    "            index = np.random.choice(len(self.allowed_states))\n",
    "        else:\n",
    "            p = self.reset_policy(epsilon)\n",
    "            index = np.random.choice(len(self.allowed_states), p=p)\n",
    "\n",
    "        self.current_pos = np.asarray(self.allowed_states[index])\n",
    "\n",
    "        # initialize the visited positions\n",
    "        self.visited = set()\n",
    "        self.visited.add(tuple(self.current_pos))\n",
    "\n",
    "        return self.state()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        \"\"\"\n",
    "        The function updates the current state with respect to the given action\n",
    "        Args:\n",
    "            action - the selected action\n",
    "        Return:\n",
    "            list[maze state, reward, is game on]\n",
    "        \"\"\"\n",
    "        is_game_on = True\n",
    "\n",
    "        # each move costs -0.05\n",
    "        reward = -0.05\n",
    "\n",
    "        move = self.action_map[action]\n",
    "        next_pos = self.current_pos + np.asarray(move)\n",
    "\n",
    "        # if the goal has been reached, the agent get a reward of 1\n",
    "        if (self.current_pos == self.goal).all():\n",
    "            reward = 1\n",
    "            is_game_on = False\n",
    "            return [self.state(), reward, is_game_on]\n",
    "        else:\n",
    "            # if the cell has been visited before, the agent get a reward of -0.2\n",
    "            if tuple(self.current_pos) in self.visited:\n",
    "                reward = -0.2\n",
    "\n",
    "        # if the move goes out of the maze or to a wall, the agent get a reward of -1\n",
    "        if self.is_state_valid(next_pos):\n",
    "            # change the current pos\n",
    "            self.current_pos = next_pos\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        self.visited.add(tuple(self.current_pos))\n",
    "        return [self.state(), reward, is_game_on]\n",
    "\n",
    "    def state(self):\n",
    "        \"\"\"\n",
    "        The function returns the state to be feeded to the network\n",
    "        Return:\n",
    "            state\n",
    "        \"\"\"\n",
    "        state = copy.deepcopy(self.maze)\n",
    "        state[tuple(self.current_pos)] = 2\n",
    "        return state\n",
    "\n",
    "    def check_boundaries(self, pos):\n",
    "        \"\"\"\n",
    "        The function checks the boundaries\n",
    "        Args:\n",
    "            pos - the position to check\n",
    "        Return:\n",
    "            bool, whether or not the position is in boundaries\n",
    "        \"\"\"\n",
    "        out = len([n for n in pos if n < 0])\n",
    "        out += len([n for n in (self.bound - np.asarray(pos)) if n <= 0])\n",
    "        return out > 0\n",
    "\n",
    "    def check_wall(self, pos):\n",
    "        \"\"\"\n",
    "        The function checks if the given position is a wall\n",
    "        Args:\n",
    "            pos - the position to check\n",
    "        Return:\n",
    "            bool, whether or not the position is a wall\n",
    "        \"\"\"\n",
    "        return self.maze[tuple(pos)] == 1\n",
    "\n",
    "    def is_state_valid(self, pos):\n",
    "        \"\"\"\n",
    "        The function checks if the given position is valid pos\n",
    "        Args:\n",
    "            pos - the position to check\n",
    "        Return:\n",
    "            bool, whether or not the position is valid\n",
    "        \"\"\"\n",
    "        if self.check_boundaries(pos):\n",
    "            return False\n",
    "        if self.check_wall(pos):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def draw(self, filename):\n",
    "        \"\"\"\n",
    "        The function draw some results from the\n",
    "        Args:\n",
    "            filename - the filename to save the image\n",
    "        \"\"\"\n",
    "        plt.Figure()\n",
    "        im = plt.imshow(self.maze, interpolation=\"none\",\n",
    "                        aspect=\"equal\", cmap=\"Greys\")\n",
    "        ax = plt.gca()\n",
    "\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        ax.plot(self.goal[1], self.goal[0], \"bs\", markersize=4)\n",
    "        ax.plot(self.current_pos[1], self.current_pos[0], \"rs\", markersize=4)\n",
    "        plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExperienceReply Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        states, actions, next_states, rewards, isgameon = zip(\n",
    "            *[self.memory[idx] for idx in indices])\n",
    "\n",
    "        return torch.Tensor(states).type(torch.float).to(device), torch.Tensor(actions).type(torch.long).to(device), torch.Tensor(next_states).to(device), torch.Tensor(rewards).to(device), torch.tensor(isgameon).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_nn(nn.Module):\n",
    "    def __init__(self, input_dim, hiddens: list, output_dim=4):\n",
    "        super(fc_nn, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hiddens[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hiddens[0], hiddens[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hiddens[1], output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class conv_nn(nn.Module):\n",
    "\n",
    "    channels = [16, 32, 64]\n",
    "    kernels = [3, 3, 3]\n",
    "    strides = [1, 1, 1]\n",
    "    in_channels = 1\n",
    "\n",
    "    def __init__(self, rows, cols, n_act):\n",
    "        super().__init__()\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels=self.in_channels,\n",
    "                                            out_channels=self.channels[0],\n",
    "                                            kernel_size=self.kernels[0],\n",
    "                                            stride=self.strides[0]),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv2d(in_channels=self.channels[0],\n",
    "                                            out_channels=self.channels[1],\n",
    "                                            kernel_size=self.kernels[1],\n",
    "                                            stride=self.strides[1]),\n",
    "                                  nn.ReLU()\n",
    "                                  )\n",
    "\n",
    "        size_out_conv = self.get_conv_size(rows, cols)\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(size_out_conv, rows*cols*2),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(rows*cols*2, int(rows*cols/2)),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(int(rows*cols/2), n_act),\n",
    "                                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(len(x), self.in_channels, self.rows, self.cols)\n",
    "        out_conv = self.conv(x).view(len(x), -1)\n",
    "        out_lin = self.linear(out_conv)\n",
    "        return out_lin\n",
    "\n",
    "    def get_conv_size(self, x, y):\n",
    "        out_conv = self.conv(torch.zeros(1, self.in_channels, x, y))\n",
    "        return int(np.prod(out_conv.size()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_loss(batch, net, device, gamma=0.99):\n",
    "    \"\"\"\n",
    "    The function compute the Q loss of the given batch\n",
    "    \"\"\"\n",
    "    # unpack the batch\n",
    "    states, actions, next_states, rewards, _ = batch\n",
    "    l_batch = len(states)\n",
    "    state_action_values = net(states.view(l_batch, -1))\n",
    "    state_action_values = state_action_values.gather(1, actions.unsqueeze(-1))\n",
    "    state_action_values = state_action_values.squeeze(-1)\n",
    "\n",
    "    next_state_values = net(next_states.view(l_batch, -1))\n",
    "    next_state_values = next_state_values.max(1)[0]\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * gamma + rewards\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the maze and define the environment\n",
    "maze = np.load(\"maze_generator/maze.npy\")\n",
    "\n",
    "initial_position = [0, 0]\n",
    "goal = [len(maze)-1, len(maze)-1]\n",
    "\n",
    "maze_env = MazeEnvironment(maze, initial_position, goal)\n",
    "# print and save the maze\n",
    "maze_env.draw(os.path.join(SAVE_PATH, \"maze_20.pdf\"))\n",
    "\n",
    "# define the agent and the buffer for the experience reply object\n",
    "buffer_capacity = 10_000\n",
    "buffer_start_size = 1_000\n",
    "memory_buffer = ExperienceReplay(buffer_capacity)\n",
    "\n",
    "agent = Agent(maze_env, memory_buffer, device, use_softmax=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 24\n",
    "gamma = 0.9\n",
    "net = fc_nn(maze.size, [maze.size] * 2, 4)\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10_000\n",
    "cutoff = 3_000\n",
    "epsilon = np.exp(-np.arange(num_epochs) / (cutoff))\n",
    "epsilon[epsilon > epsilon[100*int(num_epochs/cutoff)]\n",
    "        ] = epsilon[100*int(num_epochs/cutoff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the epsilon profile and plot the resetting probability\n",
    "plt.title(\"Epsilon Value per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Epsilon\")\n",
    "plt.plot(epsilon, ls=\"--\")\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(SAVE_PATH, \"epsilon_profile.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp, mpm = [], []\n",
    "reg = 200  # the regularization\n",
    "for e in epsilon:\n",
    "    a = agent.env.reset_policy(e)\n",
    "    mp.append(np.min(a))\n",
    "    mpm.append(np.max(a))\n",
    "\n",
    "plt.title(\"Epsilon Profile and Probability Difference per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(r\"max $p^r$ - min $p^r$\")\n",
    "plt.plot(epsilon / 1.3, ls=\"--\", alpha=0.5,\n",
    "         label=\"Epsilon Profile (arbitrary unit)\")\n",
    "plt.plot(np.array(mpm) - np.array(mp), label=\"Probability Difference\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(SAVE_PATH, \"reset_policy.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "loss_log = []\n",
    "best_loss = 1e-5\n",
    "running_loss = 0\n",
    "estop = -1\n",
    "for epoch in range(num_epochs):\n",
    "    loss = 0\n",
    "    counter = 0  # number of moves\n",
    "    eps = epsilon[epoch]\n",
    "\n",
    "    # set the is_game_on to True\n",
    "    agent.is_game_on = True\n",
    "    _ = agent.env.reset(eps)\n",
    "\n",
    "    while agent.is_game_on:\n",
    "        agent.make_move(net, eps, device)\n",
    "        counter += 1\n",
    "\n",
    "        if len(agent.buffer) < buffer_start_size:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch = agent.buffer.sample(batch_size, device)\n",
    "        loss_t = Q_loss(batch, net, device, gamma=gamma)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += loss_t.item()\n",
    "\n",
    "    if (agent.env.current_pos == agent.env.goal).all():\n",
    "        result = \"won\"\n",
    "    else:\n",
    "        result = \"lost\"\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        s_p = os.path.join(SOL_PATH, f\"sol_epoch_{epoch}.pdf\")\n",
    "        agent.plot_policy_map(net, s_p, [0.35, -0.3])\n",
    "\n",
    "    loss_log.append(loss)\n",
    "\n",
    "    if epoch > 2000:\n",
    "        running_loss = np.mean(loss_log[-50:])\n",
    "        if running_loss < best_loss:\n",
    "            print(\"saving model...\")\n",
    "            best_loss = running_loss\n",
    "            # save the model\n",
    "            torch.save(net.state_dict(), os.path.join(\n",
    "                MODEL_PATH, \"best_model.torch\"))\n",
    "            estop = epoch\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, number of moves {counter}\")\n",
    "    print(f\"Game result: {result}\")\n",
    "\n",
    "    print(\"\\t Average loss: \" + f\"{loss:.5f}\")\n",
    "    if (epoch > 2000):\n",
    "        print(\"\\t Best average loss of the last 50 epochs: \" +\n",
    "              f\"{best_loss:.5f}\" + \", achieved at epoch\", estop)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model after the training is done\n",
    "torch.save(net.state_dict(), os.path.join(MODEL_PATH, \"net.torch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results of the training\n",
    "plt.plot(epsilon * 90, alpha=0.6, ls=\"--\",\n",
    "         label=\"Epsilon profile (arbitrary unit)\")\n",
    "plt.plot((np.array(mpm) - np.array(mp)) * 120, alpha=0.6, ls=\"--\",\n",
    "         label=\"Probability difference (arbitrary unit)\")\n",
    "plt.plot(loss_log, label=\"Loss\")\n",
    "plt.title(\"Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(SAVE_PATH, \"loss.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the maze solution and the policy learnt\n",
    "net.eval()\n",
    "agent.is_game_on = True\n",
    "agent.use_softmax = False\n",
    "_ = agent.env.reset(0)\n",
    "counter = 0\n",
    "while agent.is_game_on:\n",
    "    agent.make_move(net, 0)\n",
    "    if counter % 10 == 0:\n",
    "        agent.env.draw(os.path.join(SAVE_PATH, f\"step_{counter}.pdf\"))\n",
    "    counter += 1\n",
    "agent.env.draw(os.path.join(SAVE_PATH, f\"step_{counter}.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the policy map\n",
    "agent.plot_policy_map(net, os.path.join(\n",
    "    SAVE_PATH, \"solution.pdf\"), [0.35, -0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = copy.deepcopy(net)\n",
    "best_net.load_state_dict(torch.load(os.path.join(MODEL_PATH, \"best_model.torch\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_policy_map(best_net, os.path.join(\n",
    "    SAVE_PATH, \"best_solution.pdf\"), [0.35, -0.3])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "446d6ff0a532d0fe639bf2563825411c7565c7b18e468f7e3d9d128d82feceb7"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('deep_learn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
